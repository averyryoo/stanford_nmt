{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; In 4 minutes , atmospheric chemist Rac...</td>\n",
       "      <td>&lt;start&gt; Trong 4 phút , chuyên gia hoá học khí ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; I &amp;apos;d like to talk to you today ab...</td>\n",
       "      <td>&lt;start&gt; Tôi muốn cho các bạn biết về sự to lớn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; Headlines that look like this when the...</td>\n",
       "      <td>&lt;start&gt; Có những dòng trông như thế này khi bà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; They are both two branches of the same...</td>\n",
       "      <td>&lt;start&gt; Cả hai đều là một nhánh của cùng một l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; Recently the headlines looked like thi...</td>\n",
       "      <td>&lt;start&gt; Các tiêu đề gần đây trông như thế này ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  <start> In 4 minutes , atmospheric chemist Rac...   \n",
       "1  <start> I &apos;d like to talk to you today ab...   \n",
       "2  <start> Headlines that look like this when the...   \n",
       "3  <start> They are both two branches of the same...   \n",
       "4  <start> Recently the headlines looked like thi...   \n",
       "\n",
       "                                              target  \n",
       "0  <start> Trong 4 phút , chuyên gia hoá học khí ...  \n",
       "1  <start> Tôi muốn cho các bạn biết về sự to lớn...  \n",
       "2  <start> Có những dòng trông như thế này khi bà...  \n",
       "3  <start> Cả hai đều là một nhánh của cùng một l...  \n",
       "4  <start> Các tiêu đề gần đây trông như thế này ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en = pd.read_table('data/train/train.en', header=0, names=['source'])\n",
    "data_vi = pd.read_table('data/train/train.vi', header=0, names=['target'])\n",
    "data = pd.concat([data_en,data_vi], axis=1)\n",
    "data.source = '<start> ' + data.source + ' <end>'\n",
    "data.target = '<start> ' + data.target + ' <end>'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, limit_size=None):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    lines = ['<s> ' + line + ' </s>' for line in tqdm(lines[:limit_size])]\n",
    "    #lines = [line for line in tqdm(lines[:limit_size])]\n",
    "\n",
    "    # Print examples\n",
    "    for line in lines[:5]:\n",
    "        print(line)\n",
    "\n",
    "    return lines \n",
    "\n",
    "\n",
    "def tokenize(text, vocab, max_len):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<unk>')\n",
    "\n",
    "    lang_tokenizer.word_index = vocab\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(text)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=max_len, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "\n",
    "def load_dataset(path, max_len, limit_size=None, lang=['en', 'vi']):\n",
    "    dataset_train_input_path = 'train.{}'.format(lang[0]) \n",
    "    dataset_train_target_path = 'train.{}'.format(lang[1])\n",
    "\n",
    "    print('Loading...')\n",
    "    vocab_input = load_vocab(path, lang[0])\n",
    "    vocab_target = load_vocab(path, lang[1])\n",
    "    \n",
    "    input_text = create_dataset(path + dataset_train_input_path, limit_size)\n",
    "    target_text = create_dataset(path + dataset_train_target_path, limit_size)\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    input_tensor, input_lang_tokenizer = tokenize(input_text, vocab_input, max_len)\n",
    "    target_tensor, target_lang_tokenizer = tokenize(target_text, vocab_target, max_len)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenizer, target_lang_tokenizer\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "    \n",
    "def load_dataset_test(path):\n",
    "\n",
    "    it, tt, ilt, tlt = load_dataset(path, 90, 5000)\n",
    "    print(tt[0].shape)\n",
    "    print(it.shape, tt.shape)\n",
    "    max_it, max_tt = max_length(it), max_length(tt)\n",
    "    print(max_it, max_tt)\n",
    "\n",
    "\n",
    "def load_vocab(path, lang):\n",
    "    lines = io.open(path + '.{}'.format(lang), encoding='UTF-8').read().strip().split('\\n')\n",
    "    vocab = {}\n",
    "    \n",
    "    # 0 is padding\n",
    "    for idx, word in enumerate(lines):\n",
    "        vocab[word] = idx + 1\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def convert_vocab(tokenizer, vocab):\n",
    "    for key, val in vocab.items():\n",
    "        tokenizer.index_word[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vocab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
